name: C++ CI

on:
  push:
    branches: [ master, main ]
    paths:
      - 'cpp/**'
      - 'benchmarks/**'
      - '.github/workflows/cpp-ci.yml'
  pull_request:
    branches: [ master, main ]
    paths:
      - 'cpp/**'
      - 'benchmarks/**'
      - '.github/workflows/cpp-ci.yml'

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake g++ python3-dev
    
    - name: Configure CMake
      run: |
        cd cpp
        mkdir -p build
        cd build
        cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTS=ON -DBUILD_BENCHMARKS=ON
    
    - name: Build
      run: |
        cd cpp/build
        make -j$(nproc)
    
    - name: Run C++ tests
      run: |
        cd cpp/build
        ./tests/test_drawing --gtest_output=xml:test_results.xml
    
    - name: Run C++ benchmarks (if built)
      run: |
        cd cpp/build
        if [ -f benchmarks/benchmark_drawing ]; then
          echo "Running C++ benchmarks..."
          ./benchmarks/benchmark_drawing --benchmark_out=benchmark_results.json --benchmark_out_format=json
        else
          echo "C++ benchmarks not built, skipping..."
        fi
      continue-on-error: true
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: cpp-test-results
        path: |
          cpp/build/test_results.xml
          cpp/build/benchmark_results.json
  
  python-bindings:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y g++ python3-dev
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pybind11 setuptools wheel pytest
    
    - name: Build Python bindings
      run: |
        cd cpp
        python setup.py build_ext --inplace
    
    - name: Test Python bindings
      run: |
        python test_cpp_bindings.py
        python test_batch_operations.py
    
  performance-check:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y g++ python3-dev cmake
        python -m pip install --upgrade pip
        pip install pybind11 setuptools wheel
    
    - name: Build everything
      run: |
        cd cpp
        mkdir -p build
        cd build
        cmake .. -DCMAKE_BUILD_TYPE=Release
        make -j$(nproc)
        cd ..
        python setup.py build_ext --inplace
    
    - name: Run performance checks
      run: |
        cd cpp/build
        echo "Running C++ performance tests..."
        ./tests/test_drawing --gtest_filter="PerformanceTest.*:BatchOperationsTest.Performance*"
        
        cd ../..
        echo "Running Python binding performance tests..."
        python -c "from python.drawing_cpp_wrapper import compare_performance; compare_performance()"
    
    - name: Run benchmarking suite
      run: |
        echo "Running comprehensive benchmarks..."
        python benchmarks/simple_benchmark.py
        
        echo "Running spatial benchmarks..."
        python benchmarks/spatial_benchmark.py
    
    - name: Verify performance targets
      run: |
        # Parse benchmark results and verify targets
        python -c "
import json
with open('benchmark_results_simple.json', 'r') as f:
    results = json.load(f)

# Check memory usage
creation_results = [r for r in results if r['benchmark'] == 'creation']
if creation_results:
    largest = max(creation_results, key=lambda x: x['num_objects'])
    bytes_per_obj = largest['memory_bytes'] / largest['num_objects']
    assert bytes_per_obj <= 100, f'Memory usage {bytes_per_obj} exceeds 100 bytes/object'
    print(f'✓ Memory usage: {bytes_per_obj:.1f} bytes/object')

# Check creation speed
best_creation = max([r for r in results if r['benchmark'] == 'creation'], 
                   key=lambda x: x['objects_per_sec'])
assert best_creation['objects_per_sec'] > 1e6, 'Creation speed below 1M objects/sec'
print(f'✓ Creation speed: {best_creation[\"objects_per_sec\"]/1e6:.1f}M objects/sec')

# Check save/load speed for 1M objects
save_1m = [r for r in results if r['benchmark'] == 'save_binary' and r['num_objects'] == 1000000]
if save_1m:
    assert save_1m[0]['time_ms'] < 50, f'Save time {save_1m[0][\"time_ms\"]}ms exceeds 50ms'
    print(f'✓ Save 1M objects: {save_1m[0][\"time_ms\"]:.1f}ms')

# Check batch operations
batch_results = [r for r in results if 'batch' in r['benchmark']]
if batch_results:
    best_batch = max(batch_results, key=lambda x: x['objects_per_sec'])
    assert best_batch['objects_per_sec'] > 10e6, 'Batch operations below 10M objects/sec'
    print(f'✓ Batch operations: {best_batch[\"objects_per_sec\"]/1e6:.1f}M objects/sec')

print('\\n✅ All performance targets verified!')
"
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results-${{ matrix.python-version }}
        path: |
          benchmark_results_simple.json
          benchmark_results.json
          benchmark_report.md